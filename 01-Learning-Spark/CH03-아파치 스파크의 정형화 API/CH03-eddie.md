# INTRODUCTION
- ë°ì´í„° í”„ë ˆì„ì´ ë‚˜ì˜¤ê²Œëœ ë°°ê²½  
- ìŠ¤íŒŒí¬ SQLì—”ì§„
<br><br>

# 01.RDD 
## (1)ë°œìƒë°°ê²½

### Hadoop-MapReduceì˜ í•œê³„ :
- Hadoop(í•˜ë‘¡) : ë¶„ì‚° ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬, 2006ë…„ Yahooê°€ Google ë…¼ë¬¸ ë³´ê³  ë§Œë“  ì˜¤í”ˆì†ŒìŠ¤
  - êµ¬ì„±ìš”ì†Œ
    - HDFS (Hadoop Distributed File System)
    - YARN (Yet Another Resource Negotiator) : í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
    - MapReduce ì—”ì§„
- MapReduce(ë§µë¦¬ë“€ìŠ¤): ë°ì´í„°ë¥¼ ë¶„ì‚° ì²˜ë¦¬í•˜ëŠ” ë°©ì‹, í•˜ë‘¡ì—ì„œ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„, 2004ë…„ Google ë…¼ë¬¸ì—ì„œ ë°œí‘œí•œ ë¶„ì‚° ì²˜ë¦¬ íŒ¨ëŸ¬ë‹¤ì„
  - Map í•¨ìˆ˜: ë°ì´í„°ë¥¼ key-value ìŒìœ¼ë¡œ ë³€í™˜
  - Reduce í•¨ìˆ˜: ê°™ì€ keyì˜ valueë“¤ì„ ì§‘ê³„
  - Sparkì—ì„œë„ map(), reduce() ì“°ë©´ MapReduce íŒ¨ëŸ¬ë‹¤ì„ ì‚¬ìš©í•¨  
- MapReduce ì˜ˆì‹œ
```
ì…ë ¥: ë¬¸ì„œë“¤

Map: (ë¬¸ì„œ) â†’ (ë‹¨ì–´, 1) ìŒë“¤ ìƒì„±
Shuffle: ê°™ì€ ë‹¨ì–´ë¼ë¦¬ ëª¨ìŒ
Reduce: (ë‹¨ì–´, [1,1,1...]) â†’ (ë‹¨ì–´, í•©ê³„)

ì¶œë ¥: ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜
```
- í•œê³„ì (ì„±ëŠ¥)
  - ë§¤ ì‘ì—…ë§ˆë‹¤ ë””ìŠ¤í¬ I/O ë°œìƒ (Map ê²°ê³¼ë¥¼ HDFSì— ì“°ê³ , Reduceê°€ ì½ìŒ)
  - ë°˜ë³µ ì‘ì—…(ë¨¸ì‹ ëŸ¬ë‹, ê·¸ë˜í”„ ì²˜ë¦¬)ì—ì„œ ì—„ì²­ë‚œ ì„±ëŠ¥ ì €í•˜
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ì•ˆí•¨ -> ì¤‘ê°„ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹±í•  ë°©ë²•ì´ ì—†ìŒ
  - ì¤‘ê°„ì— ë»‘ë‚˜ë©´ ë. ë‹¤ì‹œ ëŒë ¤ì•¼í•¨  
<br><br>

### Spark-RDD-DataFrameê¹Œì§€ :   
> í•˜ë‘¡ì˜ ì²˜ë¦¬ë°©ì‹ ì—”ì§„ì¸ ë§µë¦¬ë“€ìŠ¤ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸°ìœ„í•´ sparkê°€ ë‚˜ì˜´.  
> í•˜ë‘¡ì˜ ë²„ì „ì—…ì´ë¼ê³ í•˜ê¸°ì—” ì²˜ë¦¬ ì—”ì§„ì´ ì™„ì „íˆ ë‹¬ë¼ì¡Œìœ¼ë¯€ë¡œ ìƒˆë¡œìš´ í”„ë ˆì„ì›Œí¬ ëª…ì¹­ì„ ë‹¬ê³  ë‚˜ì˜¨ë“¯   
> í•˜ì§€ë§Œ í•˜ë‘¡ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ HDFS,Yarnì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©ê°€ëŠ¥í•˜ê²Œ ë§Œë“¬

<details>
  <summary>Dataframeì˜ ë°°ê²½</summary>

  - Dataframeì˜ ì‹œì‘
  ```   
  Rì˜ data.frame (1990ë…„ëŒ€) â†’ Pandas DataFrame (2008) â†’ Spark DataFrame (2014)
  ```
  - Dataframeì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ 
  ```
  - í–‰ë ¬(matrix)ê³¼ ë‹¬ë¦¬ ì»¬ëŸ¼ë§ˆë‹¤ ë‹¤ë¥¸ íƒ€ì… ê°€ëŠ¥
  - í†µê³„ ë¶„ì„ì— ìµœì í™”ëœ êµ¬ì¡°
  - SQLì²˜ëŸ¼ subset, merge, aggregate ê°€ëŠ¥
  ```
  ë~
  ---
  <br><br> 
  
</details>




- Sparkì˜ í•´ê²°ì±…
  - HDFS ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²€ì¦ëœ ë¶„ì‚° íŒŒì¼ì‹œìŠ¤í…œ)
  - YARNìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ê³µìœ  (MapReduceì™€ Spark ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥)
  - MapReduce ì—”ì§„ë§Œ êµì²´ (Spark ì—”ì§„ìœ¼ë¡œ) 
<br> 

**ì •ë¦¬í•˜ìë©´,**
- Sparkê°€ Hadoopì—ì„œ ê°€ì ¸ì˜¨ ê²ƒ
  - HDFS (ë¶„ì‚° íŒŒì¼ ì €ì¥)
  - YARN (ë¦¬ì†ŒìŠ¤ ê´€ë¦¬, ì„ íƒì‚¬í•­)

- Sparkê°€ ë²„ë¦° ê²ƒ
  - MapReduce ì‹¤í–‰ ì—”ì§„
  - ë””ìŠ¤í¬ ê¸°ë°˜ shuffle
  - Job ë‹¨ìœ„ ì‹¤í–‰ ëª¨ë¸

- Sparkê°€ ìƒˆë¡œ ë§Œë“  ê²ƒ
  - RDD ì¶”ìƒí™” -> Dataframeìœ¼ë¡œ ìƒìœ„ APIë¡œ ê°ì‹¸ì„œ ì—…ê·¸ë ˆì´ë“œ ë¨   
  - DAG ê¸°ë°˜ ì‹¤í–‰ ì—”ì§„
  - ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬
  - í†µí•© ì• í”Œë¦¬ì¼€ì´ì…˜ ëª¨ë¸
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Hadoop        â”‚     Spark        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HDFS            â”‚ (HDFS ê·¸ëŒ€ë¡œ)      â”‚
â”‚ YARN            â”‚ (YARN ì„ íƒì‚¬ìš©)    â”‚
â”‚ MapReduce ì—”ì§„   â”‚ Spark Core ì—”ì§„   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
## (2) RDDì˜ë¯¸
```
RDD
- Resilient (íšŒë³µë ¥) : íŒŒí‹°ì…˜ì´ ì†ì‹¤ë˜ë©´ lineage(ê³„ë³´) ì •ë³´ë¡œ ì¬ê³„ì‚°
- Distributed (ë¶„ì‚°) : ë°ì´í„°ê°€ ì—¬ëŸ¬ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ë‰˜ì–´ í´ëŸ¬ìŠ¤í„° ë…¸ë“œë“¤ì— ë¶„ì‚°
- Dataset (ë°ì´í„°ì…‹) : ì½ê¸° ì „ìš©(immutable) ì»¬ë ‰ì…˜
```
- Sparkì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ë°ì´í„° ì¶”ìƒí™” ë‹¨ìœ„.
- ë¶ˆë³€(immutable)í•œ ë¶„ì‚° ê°ì²´ ì»¬ë ‰ì…˜ 
- ê° RDDëŠ” ì—¬ëŸ¬ ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ë‰˜ë©°, ì´ íŒŒí‹°ì…˜ë“¤ì€ í´ëŸ¬ìŠ¤í„°ì˜ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤.
- RDDëŠ” Python, Java, Scalaì˜ ì–´ë–¤ íƒ€ì…ì˜ ê°ì²´ë¼ë„ ë‹´ì„ ìˆ˜ ìˆë‹¤.

### SparkContext vs SparkSession
**2010ë…„ ì´ˆê¸° Spark**
- SparkContextë§Œ ì¡´ì¬
- RDDë§Œ ì‚¬ìš©
- DataFrame ì—†ìŒ

**2014ë…„ ì´í›„**
- SparkSession ì¶”ê°€ (DataFrameìš©)
- SparkContextëŠ” SparkSession ë‚´ë¶€ì— í¬í•¨
- í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
  
**RDD ì‚¬ìš© ë°©ë²•1:**
```
from pyspark import SparkContext, SparkConf

# SparkContext ì§ì ‘ ìƒì„±
conf = SparkConf().setAppName("RDD Only").setMaster("local[*]")
sc = SparkContext(conf=conf)

# RDD ì‘ì—…
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2).collect()
print(result)

sc.stop()
```
ì£¼ì˜
- SparkContextëŠ” í”„ë¡œì„¸ìŠ¤ë‹¹ 1ê°œë§Œ ê°€ëŠ¥  
- ì¤‘ë³µ ìƒì„±í•˜ë©´ ì—ëŸ¬
<br>

**RDD ì‚¬ìš© ë°©ë²•2:**
```
from pyspark.sql import SparkSession

# SparkSession ìƒì„±
spark = SparkSession.builder \
    .master("local[*]") \
    .getOrCreate()

# SparkContext ì¶”ì¶œ
sc = spark.sparkContext

# RDDë§Œ ì‚¬ìš©
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2).collect()
print(result)

spark.stop()
```
## (3) RDDí•œê³„
**RDD Lazy Evaluation**
```
- ìµœì í™” ì•„ë‹˜
- ì‹¤í–‰ ì§€ì—°ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ê³„ì‚° íšŒí”¼
- ì¡°ê¸° ì¢…ë£Œë¡œ ë°ì´í„° ëœ ì½ê¸°
- Lineageë¡œ fault tolerance
```
**í•œê³„**
```
- ë¸”ë™ë°•ìŠ¤ í•¨ìˆ˜ë¼ ë‚´ë¶€ ëª¨ë¦„
- ì—°ì‚° ìˆœì„œ ê·¸ëŒ€ë¡œ ì‹¤í–‰
- ì‚¬ìš©ìê°€ ì§ì ‘ ìµœì í™” í•„ìš”
```
<br>

**DataFrameì˜ ì°¨ì´**
```
Lazy + Catalyst ìµœì í™”
ì¿¼ë¦¬ ë¶„ì„í•´ì„œ ìë™ ì¬ë°°ì¹˜
10~100ë°° ë¹ ë¦„
```
## (4) RDDíŠ¹ì§•
- RDD : Spark 1.0ë¶€í„° ìŠ¤íŒŒí¬ì— ë„ì…ëœ ê°€ì¥ ê¸°ì´ˆì ì¸ ë°ì´í„°êµ¬ì¡°
- RDDì˜ ì„¸ ê°€ì§€ í•µì‹¬ íŠ¹ì§•
    - ì˜ì¡´ì„±(dependency) : ì–´ë–¤ ì…ë ¥ì´ í•„ìš”í•˜ê³  ìƒì„±ë˜ëŠ” RDDê°€ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì§€ëŠ”ì§€ì— ëŒ€í•œ ì •ë³´
    - íŒŒí‹°ì…˜(partition)(ì§€ì—­ì„± ì •ë³´ í¬í•¨) : ì‘ì—…ì„ ë‚˜ëˆ„ì–´ ì´ê·¸ì œíë”ë“¤ì— ë¶„ì‚°í•´ íŒŒí‹°ì…˜ë³„ë¡œ ë³‘ë ¬ ì—°ì‚°í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ ë¶€ì—¬. ë§Œì•½ íŒŒì¼ì„ ì½ëŠ” ê²½ìš° ê° ì´ê·¸ì œíí„°ê°€ ê°€ê¹Œì´ ìˆëŠ” ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ê·¸ì œíí„°ì—ê²Œ ìš°ì„ ì ìœ¼ë¡œ ì‘ì—…ì„ ë³´ëƒ„.
    - ì—°ì‚° í•¨ìˆ˜(compute function) : ì €ì¥ëœ ë°ì´í„°ë¥¼ Iterator[T] í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤Œ.

## (5) RDDì™€ ë°ì´í„°í”„ë ˆì„ ì‹¤í–‰
- RDD íŒŒì´ì¬ì˜ˆì œ(ì €ìˆ˜ì¤€)
```
sc = spark.sparkContext
# (name, age) í˜•íƒœì˜ íŠœí”Œë¡œ ëœ RDD ìƒì„±
dataRDD = sc.parallelize([("Brooke", 20), ("Denny", 31), ("Jules", 30), ("TD", 35), ("Brooke", 25)])
# ì§‘ê³„ì™€ í‰ê· ì„ ìœ„í•œ ëŒë‹¤ í‘œí˜„ì‹, map, reduceByKey transformation
ageRDD = (dataRDD
          .map(lambda x: (x[0], (x[1], 1)))
          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
          .map(lambda x: (x[0], x[1][0] / x[1][1]))
          )
print(ageRDD.collect())
```
<br>

- DataFrame ì˜ˆì œ(ê³ ìˆ˜ì¤€)
```
data_df = spark.createDataFrame([("Brooke", 20), ("Denny", 31), ("Jules", 30), ("TD", 35), ("Brooke", 25)], ['name', 'age'])
avg_df = data_df.groupBy('name').agg(avg('age'))
avg_df.show()
```
---
<br><br><br>

# 02. ë°ì´í„°í”„ë ˆì„ API

## (1) ë°ì´í„°í”„ë ˆì„
```
  Rì˜ data.frame (1990ë…„ëŒ€) â†’ Pandas DataFrame (2008) â†’ Spark DataFrame (2014)
```
- ë¶„ì‚° ì¸ë©”ëª¨ë¦¬ í…Œì´ë¸”ì²˜ëŸ¼ ë™ì‘  
- ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì… ì§€ì • ê°€ëŠ¥ â†’ RDDëŠ” íƒ€ì…ì •ë³´ ì—†ì—ˆìŒ.  
- ë¶ˆë³€ì„± - ì›ë³¸ìœ ì§€ ex) replace=TRUE

## (2) êµ¬ì¡°
### êµ¬ì„±ìš”ì†Œ
- ìŠ¤í‚¤ë§ˆ  
- ë°ì´í„°  
```
ìŠ¤í‚¤ë§ˆ:
- name: string
- age: int
- job: string

ë°ì´í„°:
Row(name='Alice', age=25, job='Engineer')
Row(name='Bob', age=30, job='Manager')
```
### ìƒì„±ë°©ë²•  
- ë°©ë²• 1: ë¦¬ìŠ¤íŠ¸ì—ì„œ ìƒì„±
```
data = [
    ("Brooke", 20),
    ("Denny", 31)
]

df = spark.createDataFrame(data, ["name", "age"])
```
- ë°©ë²• 2: íŒŒì¼ì—ì„œ ì½ê¸°
```
# CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Parquet
df = spark.read.parquet("data.parquet")

# JSON
df = spark.read.json("data.json")
```
-  Pandasì—ì„œ ë³€í™˜
```
import pandas as pd

pandas_df = pd.DataFrame({
    'name': ['Alice', 'Bob'],
    'age': [25, 30]
})

spark_df = spark.createDataFrame(pandas_df)
```
### ì£¼ìš” DataFrame API êµ¬ì„±
```
ìƒì„±: createDataFrame, read.csv, read.parquet
ì¡°íšŒ: select, show, printSchema
í•„í„°: filter, where
ë³€í™˜: withColumn, withColumnRenamed, drop
ì§‘ê³„: groupBy, agg, avg, sum, count
ì •ë ¬: orderBy
ê²°í•©: join
SQL: createOrReplaceTempView, sql
ì €ì¥: write.csv, write.parquet
```
<br><br>

## (2)ë°ì´í„° íƒ€ì…
- SparkëŠ” ìì²´ íƒ€ì… ì‹œìŠ¤í…œ ë³´ìœ 
- Python/Java íƒ€ì…ì„ Spark íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì²˜ë¦¬  

### 1ë‹¨ê³„: type ì‹œìŠ¤í…œ ê°œìš”

**ë¬¸ì œìƒí™©**
> "ê° ì–¸ì–´ë§ˆë‹¤ íƒ€ì…ì´ ë‹¤ë¥´ì–ì•„. SparkëŠ” ì´ ëª¨ë“  ì–¸ì–´ë¥¼ ì§€ì›í•´ì•¼ í•˜ëŠ”ë°, ì–´ë–»ê²Œ í†µì¼í•˜ì§€?"
```
Python : int, str, list
Java   : Integer, String, ArrayList
Scala  : Int, String, List
```

**í•´ê²°ì±…**
> "ì•„, Spark ìì²´ íƒ€ì… ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì„œ ì¤‘ê°„ì—ì„œ ë³€í™˜í•˜ëŠ” ê±°ë„¤."  
> "JVMì—ì„œ ì‹¤í–‰ë˜ë‹ˆê¹Œ ê²°êµ­ Java íƒ€ì… ê¸°ë°˜ì´ê¸´ í•œë°, ê° ì–¸ì–´ì—ì„œ ì‰½ê²Œ ì“¸ ìˆ˜ ìˆê²Œ ë˜í•‘í•œ ê±°ì•¼."
- Spark ìì²´ íƒ€ì… ì •ì˜
- ëª¨ë“  ì–¸ì–´ë¥¼ Spark íƒ€ì…ìœ¼ë¡œ í†µì¼
- JVMì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ Java ê¸°ë°˜

**ìŠ¤í‚¤ë§ˆì„ ì–¸ ë°©ë²•**
- 0) ìŠ¤í‚¤ë§ˆ ì„ ì–¸
	- ```
	  from pyspark.sql.types import *

		schema = StructType([
		    StructField("user_id", IntegerType(), False),
		    StructField("is_active", BooleanType(), True),
		    StructField("created_at", DateType(), True)
		])

	  ```
- 1) DataFrame ìƒì„± ì‹œ ì§ì ‘ ì§€ì •
	- ```
	  df = spark.createDataFrame(data, schema)
	  ```
- 2) íŒŒì¼ ì½ì„ ë•Œ ì§€ì • (ì‹¤ë¬´ í•µì‹¬)
	- ```
	  df = spark.read \
			    .schema(schema) \
			    .csv("s3://bucket/users.csv")
	  ```
- 3) í…Œì´ë¸”ì—ì„œ ìƒì† (ê°€ì¥ ì•ˆì •ì )
	- ```
	df = spark.read.table("analytics.users")
	  ```
- ê¸°ë³¸ì ìœ¼ë¡œ parquetì— íƒ€ì…ì´ ì„¤ì •ë˜ì–´ìˆì–´ì„œ(ê°•ì œ) ì§€ì •ì•ˆí•´ë„ë¨.  

### 2ë‹¨ê³„: ê¸°ë³¸íƒ€ì… ë° ì„¤ì •ë°©ë²•

**1) ìˆ«ìíƒ€ì…**

```
from pyspark.sql.types import *

# ì •ìˆ˜
ByteType()      # 8-bit ì •ìˆ˜ (-128 ~ 127)
ShortType()     # 16-bit ì •ìˆ˜ (-32768 ~ 32767)
âœ…IntegerType()   # 32-bit ì •ìˆ˜ (ì¼ë°˜ì  ì‚¬ìš©)
LongType()      # 64-bit ì •ìˆ˜ (í° ìˆ˜)

# ì‹¤ìˆ˜
FloatType()     # 32-bit ë¶€ë™ì†Œìˆ˜ì 
DoubleType()    # 64-bit ë¶€ë™ì†Œìˆ˜ì  (ê¶Œì¥)

# ê³ ì •ì†Œìˆ˜ì 
DecimalType(precision, scale)  # ê¸ˆìœµ ê³„ì‚°ìš©
# DecimalType(10, 2) â†’ 12345678.90
```

- ì„¤ì •íŒ
```
# ì •ìˆ˜
- ByteType, ShortType â†’ ê±°ì˜ ì•ˆ ì”€. ë©”ëª¨ë¦¬ ê·¹í•œ ìµœì í™”í•  ë•Œë§Œ.
- IntegerType â†’ ì¼ë°˜ì ì¸ ìˆ«ì (ë‚˜ì´, ê°œìˆ˜)
- LongType â†’ í° ìˆ«ì (ì‚¬ìš©ì ID, timestamp)

# ì‹¤ìˆ˜
"FloatType vs DoubleTypeëŠ”... 
- ê³¼í•™ ê³„ì‚°ì´ë©´ DoubleTypeì´ ì •ë°€ë„ ë†’ì•„ì„œ ì´ê±° ì“°ê³ ."

# ê³ ì •ì†Œìˆ˜ì 
- ë‚˜ìœ ì˜ˆ: ê¸ˆì•¡ì„ DoubleTypeìœ¼ë¡œ
	amount = DoubleType()  # 999.99 + 0.01 = 1000.0000000001 (ë²„ê·¸!)

- ì¢‹ì€ ì˜ˆ: ê¸ˆì•¡ì„ DecimalTypeìœ¼ë¡œ  
	amount = DecimalType(12, 2)  # ì •í™•íˆ 1000.00
```

**2) ë¬¸ìì—´/ë‚ ì§œ**

```
StringType()    # ë¬¸ìì—´, UTF-8 ë¬¸ìì—´ (ë¬´ì œí•œ ê¸¸ì´)
BinaryType()    # ë°”ì´íŠ¸ ë°°ì—´

BooleanType()   # True/False

DateType()      # ë‚ ì§œë§Œ (2025-01-30)
TimestampType() # ë‚ ì§œ+ì‹œê°„ (2025-01-30 15:30:00)
```

- ì„¤ì •íŒ
```
# ë‚ ì§œ
"DateType vs TimestampType
- ìƒì¼ ê°™ì€ ê±´ Date
- ë¡œê·¸ ë¶„ì„í•  ë•Œ ì‹œê°„ê¹Œì§€ í•„ìš”í•˜ë©´ Timestamp 
```

**3) ë³µí•© íƒ€ì…**
- ArrayType (ë°°ì—´)
	- "ì˜¤, array ì•ˆì— element íƒ€ì…ì´ ì •ì˜ë˜ë„¤. Python listë‘ ë¹„ìŠ·í•œë° íƒ€ì…ì´ ëª…ì‹œë˜ì–´ ìˆì–´."
```
from pyspark.sql.types import ArrayType, IntegerType

# ì •ìˆ˜ ë°°ì—´
ArrayType(IntegerType())

# ë¬¸ìì—´ ë°°ì—´
ArrayType(StringType())

data = [
    (1, [101, 102, 103]),  # user_id: 1, êµ¬ë§¤í•œ product_ids
    (2, [201])
]

df = spark.createDataFrame(data, ["user_id", "product_ids"])

df.printSchema()
# root
#  |-- user_id: long
#  |-- product_ids: array
#      |-- element: long

```
- MapType (ë”•ì…”ë„ˆë¦¬)
	- "ì‹¤ë¬´ì—ì„œ ì–¸ì œ ì“°ë‚˜ ìƒê°í•´ë³´ë©´... ì„¤ì • ê°’ì´ë‚˜ ë©”íƒ€ë°ì´í„° ì €ì¥í•  ë•Œ ìœ ìš©í•˜ê² ë„¤."
```
from pyspark.sql.types import MapType

# ë¬¸ìì—´ â†’ ì •ìˆ˜ ë§µ
MapType(StringType(), IntegerType())



data = [
    (1, {"math": 90, "english": 85}),
    (2, {"math": 75, "english": 95})
]

df = spark.createDataFrame(data, ["id", "scores"])

df.printSchema()
# root
#  |-- id: long
#  |-- scores: map
#      |-- key: string
#      |-- value: long
```
- StructType (ì¤‘ì²© êµ¬ì¡°)
	- "addressê°€ ë˜ StructTypeì´ë„¤. ì¤‘ì²© êµ¬ì¡°. JSONì´ë‘ ì™„ì „ ë˜‘ê°™ì€ êµ¬ì¡°ì•¼."
```
from pyspark.sql.types import StructType, StructField

# ìŠ¤í‚¤ë§ˆ ì •ì˜
schema = StructType([
    StructField("name", StringType(), nullable=False),
    StructField("age", IntegerType(), nullable=True),
    StructField("address", StructType([
        StructField("city", StringType()),
        StructField("zip", StringType())
    ]))
])


data = [
    ("Alice", 25, {"city": "Seoul", "zip": "12345"}),
    ("Bob", 30, {"city": "Busan", "zip": "67890"})
]

df = spark.createDataFrame(data, schema)

df.printSchema()
# root
#  |-- name: string (nullable = false)
#  |-- age: integer (nullable = true)
#  |-- address: struct
#      |-- city: string
#      |-- zip: string
```

### 3ë‹¨ê³„ ìŠ¤í‚¤ë§ˆ ì •ì˜ë°©ë²•

**ë°©ë²•1.ìë™ì¶”ë¡ **
- í•­ìƒ ì •í™•í•˜ì§€ ì•ŠìŒ
- ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ (ë°ì´í„° ìŠ¤ìº” í•„ìš”)
```
data = [("Alice", 25), ("Bob", 30)]

# Sparkê°€ ìë™ íƒ€ì… ì¶”ë¡ 
df = spark.createDataFrame(data, ["name", "age"])

df.printSchema()
# root
#  |-- name: string
#  |-- age: long (ìë™ìœ¼ë¡œ long ì„ íƒ)
```
**ë°©ë²• 2: DDL ë¬¸ìì—´**
- ê°„ë‹¨í•œ ìŠ¤í‚¤ë§ˆ
- "ì˜¤, SQL ìŠ¤íƒ€ì¼ì´ë¼ ì½ê¸° í¸í•˜ë„¤."
- "ê·¼ë° nullable ì œì–´ê°€ ì•ˆ ë˜ë„¤... ê·¸ëŸ¼ ë³µì¡í•œ ê±´ ëª» ì“°ê² ì–´."
```
# SQL DDL ìŠ¤íƒ€ì¼
schema_ddl = "name STRING, age INT, salary DOUBLE"

df = spark.createDataFrame(data, schema_ddl)
```
**ë°©ë²• 3: StructType ëª…ì‹œ**
- "ì´ê²Œ ì œì¼ ëª…í™•í•˜ê³  ì•ˆì „í•´."
- íƒ€ì… ì™„ì „ ì œì–´
- nullable ì§€ì • ê°€ëŠ¥
- í”„ë¡œë•ì…˜ ê¶Œì¥
- "user_idê°€ NULLì´ë©´ ì•ˆ ë˜ì–ì•„. ì´ëŸ° ê±° ëª…ì‹œí•´ì•¼ ë‚˜ì¤‘ì— ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ì•ˆ ìƒê²¨."
```
from pyspark.sql.types import *

schema = StructType([
    StructField("name", StringType(), nullable=False), # NOT NULL
    StructField("age", IntegerType(), nullable=True),
    StructField("salary", DoubleType(), nullable=True)
])

df = spark.createDataFrame(data, schema)

```

**ìŠ¤í‚¤ë§ˆì„ ì–¸ ì‚¬ìš©ë°©ë²•**
- 0) ìŠ¤í‚¤ë§ˆ ì„ ì–¸ - ë°©ë²•3
	- ```
	  from pyspark.sql.types import *

		schema = StructType([
		    StructField("user_id", IntegerType(), False),
		    StructField("is_active", BooleanType(), True),
		    StructField("created_at", DateType(), True)
		])

	  ```
- 1) DataFrame ìƒì„± ì‹œ ì§ì ‘ ì§€ì •
	- ```
	  df = spark.createDataFrame(data, schema)
	  ```
- 2) íŒŒì¼ ì½ì„ ë•Œ ì§€ì • (ì‹¤ë¬´ í•µì‹¬)
	- ```
	  df = spark.read \
			    .schema(schema) \
			    .csv("s3://bucket/users.csv")
	  ```
- 3) í…Œì´ë¸”ì—ì„œ ìƒì† (ê°€ì¥ ì•ˆì •ì )
	- ```
	df = spark.read.table("analytics.users")
	  ```
- ê¸°ë³¸ì ìœ¼ë¡œ parquetì— íƒ€ì…ì´ ì„¤ì •ë˜ì–´ìˆì–´ì„œ(ê°•ì œ) ì§€ì •ì•ˆí•´ë„ë¨.  


### 4ë‹¨ê³„ íƒ€ì…ë³€í™˜ 
> "CSV ì½ìœ¼ë©´ ì „ë¶€ ë¬¸ìì—´ë¡œ ì˜¤ëŠ”ë°, ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¾¸ì§€?"

- ìºìŠ¤íŒ… ì˜ˆì‹œ
```
from pyspark.sql.functions import col

df = spark.createDataFrame([
    ("1", "100"),
    ("2", "200")
], ["id", "amount"])

# ë¬¸ìì—´ â†’ ì •ìˆ˜
df = df.withColumn("id", col("id").cast(IntegerType()))
df = df.withColumn("amount", col("amount").cast(DoubleType()))

# ë˜ëŠ” ë¬¸ìì—´ë¡œ
df = df.withColumn("id", col("id").cast("int"))
df = df.withColumn("amount", col("amount").cast("double"))

df.printSchema()
# root
#  |-- id: integer
#  |-- amount: double
```


- castì‹¤íŒ¨í•˜ë©´ nullì²˜ë¦¬ë¨
	- "abc" â†’ IntegerType ìºìŠ¤íŒ… â†’ NULL
	- "NULLë¡œ ë³€í•˜ë„¤. ì¡°ìš©íˆ ì‹¤íŒ¨í•˜ë‹ˆê¹Œ í™•ì¸í•´ì•¼ê² ì–´."
```
# ë³€í™˜ í›„ ê²€ì¦ 
df.filter(col("age").cast("int").isNotNull())
```

- ê´€ë ¨ ì½”ë“œ
```
# êµ¬ì¡° í™•ì¸
df.printSchema()

# ìŠ¤í‚¤ë§ˆ ê°ì²´ ê°€ì ¸ì˜¤ê¸°
schema = df.schema
print(schema)

# íŠ¹ì • ì»¬ëŸ¼ íƒ€ì…
print(df.schema["age"].dataType)  # IntegerType


# ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ
numeric_cols = [f.name for f in df.schema.fields 
                if isinstance(f.dataType, (IntegerType, DoubleType, LongType))]

# ë¬¸ìì—´ ì»¬ëŸ¼ë§Œ
string_cols = [f.name for f in df.schema.fields 
               if isinstance(f.dataType, StringType)]
```

### 5ë‹¨ê³„ íŒŒì´ì¬ ë§¤í•‘ ì£¼ì˜ì‚¬í•­

**Python vs Spark íƒ€ì… ë§¤í•‘**
- "Python ë°ì´í„°ë¥¼ createDataFrameí•˜ë©´ ìë™ ë³€í™˜ë˜ëŠ”ë°..."
```
Python          â†’ Spark
------------------------------
None            â†’ NULL
bool            â†’ BooleanType
int             â†’ LongType (ì£¼ì˜: IntegerType ì•„ë‹˜)
float           â†’ DoubleType
str             â†’ StringType
bytes           â†’ BinaryType
datetime.date   â†’ DateType
datetime.datetime â†’ TimestampType
list            â†’ ArrayType
dict            â†’ MapType
```


**Python(int) â†’ LongType**
- "ì–´? intê°€ LongTypeì´ë„¤? IntegerType ì•„ë‹ˆê³ ?"
- "ì™œ longìœ¼ë¡œ ì¶”ë¡ í•˜ì§€? ì•„, Python intëŠ” í¬ê¸° ì œí•œ ì—†ì–´ì„œ ì•ˆì „í•˜ê²Œ LongTypeìœ¼ë¡œ ë§¤í•‘í•˜ëŠ”êµ¬ë‚˜."
- "IntegerType ì›í•˜ë©´ ëª…ì‹œí•´ì•¼ í•´."
```
data = [(1, 2), (3, 4)]
df = spark.createDataFrame(data, ["a", "b"])

df.printSchema()
# root
#  |-- a: long (int ì•„ë‹˜!)
#  |-- b: long

# ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ì£¼ê¸°
schema = StructType([
    StructField("a", IntegerType()),
    StructField("b", IntegerType())
])
```


### 6ë‹¨ê³„ null ì²˜ë¦¬ ì£¼ì˜ì‚¬í•­

**nullable Falseì—¬ë„ ì—ëŸ¬ì•ˆë‚¨**
- "nullable=Falseë¡œ í•˜ë©´ NULL ë“¤ì–´ì˜¤ë©´ ì—ëŸ¬ë‚˜ë‚˜?"
- "ì•„ë‹ˆ, ê²½ê³ ë§Œ í•˜ê³  ë°›ì•„ë“¤ì—¬... Sparkê°€ ê´€ëŒ€í•´. ê·¸ë˜ì„œ ë” ìœ„í—˜í•´."
- "ê²€ì¦í•˜ëŠ” ì¶œë ¥ë¬¸ ì‘ì„±í•´ì•¼í•´"

```
StructField("user_id", IntegerType(), nullable=False)


## ì½ì„ ë•Œ ê²€ì¦
df = spark.read.csv("data.csv", schema=schema)

# NULL ì²´í¬
null_count = df.filter(col("user_id").isNull()).count()
if null_count > 0:
    raise ValueError(f"user_idì— NULL {null_count}ê°œ ë°œê²¬!")

```

**Nullì²˜ë¦¬í•¨ìˆ˜**
- "ì‹¤ë¬´ì—ì„  fillnaë³´ë‹¤ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²Œ ë‚˜ì•„."
```
# NULL í•„í„°
df.filter(col("age").isNotNull())

# NULL ì œê±°
df.dropna(subset=["age"])

# NULL ì±„ìš°ê¸°
df.fillna({"age": 0, "name": "Unknown"})


# ëª…ì‹œì ì²˜ë¦¬
from pyspark.sql.functions import when, col

df = df.withColumn("age_clean",
    when(col("age").isNull(), 0)
    .otherwise(col("age"))
)
```

### 7ë‹¨ê³„ - ë³µì¡í•œ íƒ€ì… ë‹¤ë£¨ê¸°

**ë°°ì—´ ì»¬ëŸ¼ ì ‘ê·¼**
- "explode... ë°°ì—´ í•œ ê°œê°€ ì—¬ëŸ¬ í–‰ìœ¼ë¡œ í¼ì³ì§€ë„¤."
- "ë¡œê·¸ ë¶„ì„í•  ë•Œ ìœ ìš©í•˜ê² ì–´. í•œ ì‚¬ìš©ìê°€ ì—¬ëŸ¬ ì´ë²¤íŠ¸ ë°œìƒì‹œí‚¨ ê±° í¼ì¹  ë•Œ."
```
from pyspark.sql.functions import explode, col

data = [(1, ["A", "B", "C"])]
df = spark.createDataFrame(data, ["id", "items"])

# ë°°ì—´ ë¶„í•´ (ê° ìš”ì†Œë¥¼ í–‰ìœ¼ë¡œ)
df.select(col("id"), explode(col("items")).alias("item")).show()
# +---+----+
# | id|item|
# +---+----+
# |  1|   A|
# |  1|   B|
# |  1|   C|
# +---+----+

# ë°°ì—´ ì¸ë±ìŠ¤ ì ‘ê·¼
df.select(col("items")[0].alias("first_item")).show()
```
**Map ì»¬ëŸ¼ ì ‘ê·¼**
```
data = [(1, {"math": 90, "eng": 85})]
df = spark.createDataFrame(data, ["id", "scores"])

# í‚¤ë¡œ ì ‘ê·¼
df.select(col("scores")["math"]).show()
```
**Struct ì»¬ëŸ¼ ì ‘ê·¼**
- "JSON ê²½ë¡œ íƒìƒ‰í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì  ì°ì–´ì„œ ì ‘ê·¼. ì§ê´€ì ì´ì•¼."
```
# ì  í‘œê¸°ë²•
df.select(col("address.city"), col("address.zip"))

# getField
df.select(col("address").getField("city"))
```

---

### ìì£¼ ì“°ì´ëŠ” ë°ì´í„° í”„ë ˆì„ ì‘ì—…ë“¤

**DataFrameReaderì™€ DataFrameWriter**
- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(Json,csv,parquet,avro,orc) ì½ê¸° â†’ DataFrameReaderë¡œ í†µí•©
- ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(Json,csv,parquet,avro,orc) ì½ê¸° â†’ DataFrameWriterë¡œ í†µí•©

```python
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì˜ˆì œ - ìƒŒí”„ë€ì‹œìŠ¤ì½” ì†Œë°©ì„œ ë°ì´í„°
fire_df = spark.read.csv('sf-fire-calls.csv', header=True)
fire_df.show(5)
```

**í”„ë¡œì ì…˜ê³¼ í•„í„°**

- **í”„ë¡œì ì…˜(projection)** : íŠ¹ì • ê´€ê³„ ìƒíƒœì™€ ë§¤ì¹˜ë˜ëŠ” í–‰ë“¤ë§Œ ë˜ëŒë ¤ ì£¼ëŠ” ë°©ë²•
    - .select() ë’¤ì— .where() or .filter() ë¥¼ ì´ìš©í•˜ì—¬ ì‚¬ìš©.

```python
# CallTypeì´ "Medical Incident"ê°€ ì•„ë‹Œ ë°ì´í„° í•„í„°
from pyspark.sql.functions import col
few_fire_df = (fire_df
               .select("IncidentNumber", "AvailableDtTM", "CallType")
               .where(col("CallType") != "Medical Incident"))
few_fire_df.show(5, truncate=False)
```

```python
# CallTypeì˜ ì¢…ë¥˜ê°€ ëª‡ ê°€ì§€ì¸ì§€ ì„¸ì–´ì£¼ëŠ” ì½”ë“œ
(fire_df.select("CallType")
 .where(col("CallType").isNotNull())
 .agg(countDistinct("CallType").alias("DistinctCallTypes"))
 .show()
 )
```

```python
# ëª¨ë“  í–‰ì—ì„œ nullì´ ì•„ë‹Œ ê°œë³„ CallTypeì„ ì¶”ì¶œ
(fire_df
 .select("CallType")
 .where(col("CallType").isNotNull())
 .distinct()
 .show(10, False)
 )
```

**ì¹¼ëŸ¼ì˜ ì´ë¦„ ë³€ê²½ ë° ì¶”ê°€ ì‚­ì œ**

- withColumnRenamed()

```python
new_fire_df = fire_df.withColumnRenamed("Delay", "changeDelay")
new_fire_df.select("changeDelay").show(5)
```

<aside>
ğŸ’¡ ë°ì´í„° í”„ë ˆì„ ë³€í˜•ì€ ë³€ê²½ ë¶ˆê°€ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ë¯€ë¡œ withColumnsRenamed()ë¡œ ì¹¼ëŸ¼ ì´ë¦„ì„ ë³€ê²½í•  ë•ŒëŠ”, ê¸°ì¡´ ì¹¼ëŸ¼ ì´ë¦„ì„ ê°–ê³  ìˆëŠ” ì›ë³¸ì„ ìœ ì§€í•œ ì±„ë¡œ ì¹¼ëŸ¼ ì´ë¦„ì´ ë³€ê²½ëœ ìƒˆë¡œìš´ ë°ì´í„° í”„ë ˆì„ì„ ë°›ì•„ ì˜¤ê²Œ ëœë‹¤.

</aside>

**ì¹¼ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ë³€ê²½**

- to_timestamp(), to_date()
    - ë³€ê²½ ì½”ë“œ
    
    ```python
    fire_ts_df = (new_fire_df
                  .withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy"))
                  .drop("CallDate")
                  .withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy"))
                  .drop("WatchDate")
                  .withColumn("AvailableDtTs", to_timestamp(col("AvailableDtTm"), "MM/dd/yyyy hh:mm:ss a"))
                  .drop("AvailableDtTm"))
    ```
    
    
    - ë°ì´í„° íƒ€ì…ì´ ìˆ˜ì •ëœ ë‚ ì§œ/ì‹œê°„ ì¹¼ëŸ¼ì„ ê°€ì§€ê²Œ ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´í›„ ë°ì´í„° íƒìƒ‰ ê³¼ì •ì—ì„œ spark.sql.functionsì˜ dayofmonth(), dayofyear(), dayofweek() ì™€ ê°™ì€ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì§ˆì˜í•  ìˆ˜ ìˆë‹¤.

**ì§‘ê³„ì—°ì‚°**

- groupBy(), orderBy(), count()

```python
# CallTypeì˜ ì§‘ê³„
(fire_ts_df
 .select("CallType")
 .where(col("CallType").isNotNull())
 .groupBy("CallType")
 .count()
 .orderBy("count", ascending = False)
 .show(n = 10, truncate = False))
```

### ì •ì íƒ€ì…, ë™ì íƒ€ì… 
- scala, java - ë°ì´í„°í”„ë ˆì„, ë°ì´í„°ì„¸íŠ¸ API ì‚¬ìš©ê°€ëŠ¥
- python - ë°ì´í„°í”„ë ˆì„ API ì‚¬ìš©ê°€ëŠ¥

Spark ì½”ì–´ ì—”ì§„:
100% Scalaë¡œ ì‘ì„±ë¨
JVM ìœ„ì—ì„œ ì‹¤í–‰ë¨
Catalyst optimizer, DAG scheduler ëª¨ë‘ Scala ì½”ë“œ

ê·¸ëŸ°ë° ì™œ Pythonìœ¼ë¡œ ì“¸ ìˆ˜ ìˆë‚˜?
Pythonì€ 'í´ë¼ì´ì–¸íŠ¸'ì¼ ë¿
ì‹¤ì œ ì—°ì‚°ì€ JVMì—ì„œ ì¼ì–´ë‚¨
Python â†” JVM ì‚¬ì´ì— í†µì‹  ë ˆì´ì–´(Py4J) ì¡´ì¬


### ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ì˜µí‹°ë§ˆì´ì €

- ì—°ì‚° ì¿¼ë¦¬ë¥¼ ë°›ì•„ ì‹¤í–‰ ê³„íšìœ¼ë¡œ ë³€í™˜í•œë‹¤. ì´ëŠ” ë‹¤ìŒì˜ 4ë‹¨ê³„ë¥¼ ê±°ì¹œë‹¤.
    
1. **ë¶„ì„**
    - ì¶”ìƒ ë¬¸ë²• íŠ¸ë¦¬(abstract syntax tree) ìƒì„±
2. **ë…¼ë¦¬ì  ìµœì í™”**
    - ë¹„ìš© ê¸°ë°˜ ì˜µí‹°ë§ˆì´ì €(cost based optimizer, CBO)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ê³„íšì— ë¹„ìš©ì„ ì±…ì •. ì´ëŠ” ì—°ì‚° íŠ¸ë¦¬ë“¤ë¡œ ë°°ì—´ë¨.
    - ì¡°ê±´ì ˆ í•˜ë¶€ ë°°ì¹˜, ì¹¼ëŸ¼ ê±¸ëŸ¬ë‚´ê¸°, Boolean ì—°ì‚° ë‹¨ìˆœí™” ë“±ì„ í¬í•¨
3. **ë¬¼ë¦¬ ê³„íš ìˆ˜ë¦½**
    - ë…¼ë¦¬ ê³„íšì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€ì‘ë˜ëŠ” ë¬¼ë¦¬ì  ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ëœ ë¬¼ë¦¬ ê³„íšì„ ìƒì„±
4. **ì½”ë“œ ìƒì„±**
    - ì‹¤ì œë¡œ ì‹¤í–‰í•  ìë°” ë°”ì´íŠ¸ ì½”ë“œë¥¼ ìƒì„±
    - ì‹¤í–‰ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ì½”ë“œ ìƒì„±ì„ ìœ„í•´ ìµœì‹  ì»´íŒŒì¼ëŸ¬ ê¸°ìˆ ì„ ì‚¬ìš©í•¨. ë‹¤ì‹œ ë§í•˜ìë©´ ìŠ¤íŒŒí¬ëŠ” ì»´íŒŒì¼ëŸ¬ì²˜ëŸ¼ ë™ì‘í•œë‹¤ê³  í•  ìˆ˜ ìˆìŒ.
    - í¬ê´„ ì½”ë“œ ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” í……ìŠ¤í…ì´ ì‚¬ìš©ëœë‹¤.
    
    <aside>
    ğŸ’¡ í¬ê´„ ì½”ë“œ(whole-stage) ìƒì„±
    
    - ë¬¼ë¦¬ì  ì¿¼ë¦¬ ìµœì í™” ë‹¨ê³„ë¡œ ì „ì²´ ì¿¼ë¦¬ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ í•©ì¹˜ë©´ì„œ ê°€ìƒ í•¨ìˆ˜ í˜¸ì¶œì´ë‚˜ ì¤‘ê°„ ë°ì´í„°ë¥¼ ìœ„í•œ CPU ë ˆì§€ìŠ¤í„° ì‚¬ìš©ì„ ì—†ì• ë²„ë¦¼.
    </aside>
    




